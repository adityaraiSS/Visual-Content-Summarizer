# Visual-Content-Summarizer
Version 1:
In Version 1 of the image captioning project, a Convolutional Neural Network (CNN) combined with a Recurrent Neural Network (RNN), specifically a Long Short-Term Memory (LSTM) architecture, was implemented. This architecture aimed to convert images into descriptive captions. The project utilized the Flickr 8K dataset and achieved a loss of 2.689. The CNN extracts features from the images, and the LSTM generates captions based on these features. This version provided a baseline understanding of image captioning but had limitations in capturing intricate details and context.

Version 2:
Version 2 of the project improved upon the first iteration by implementing a CNN-RNN architecture with Attention Mechanism. This approach utilized a larger MSCOCO dataset containing 327,437 sample images, resulting in a more robust model. The Attention Mechanism allowed the model to focus on relevant parts of the image while generating captions, leading to more accurate and contextually rich descriptions. The model achieved a lower loss of 1.625 compared to Version 1. The README includes a detailed explanation of the Attention Mechanism, including the mathematical formulation and its implementation. Additionally, it provides some sample predictions generated by the model on the test dataset, showcasing its ability to generate descriptive captions accurately.
